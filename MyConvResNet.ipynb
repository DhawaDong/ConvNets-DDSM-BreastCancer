{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhawaDong/ConvsNet-DDSM/blob/main/MyConvResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5fHcgeS8Vwv"
      },
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')\n",
        "#%reset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RryJVc3zU6N0"
      },
      "source": [
        "import pandas as pd;\n",
        "from imutils import paths\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import joblib\n",
        "from imutils import paths\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8JuxwhLV-qn"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import albumentations\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "#from utils import EarlyStopping, LRScheduler\n",
        "#import openpyxl;\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "from torchvision import models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FKT4UItt8kT"
      },
      "source": [
        "#MetaData for input images.....\n",
        "#ImagePaths = pd.read_excel('/content/drive/MyDrive/ImageFolderCalcMass.xlsx');\n",
        "#ImagePaths = pd.read_excel('/content/drive/MyDrive/ImageFolderCalcMassNoBlank.xlsx');\n",
        "#Imageinput ant classes....\n",
        "#ImagePaths[\"Pathology\"].replace({\"BENIGN\": 0, \"BENIGN_WITHOUT_CALLBACK\": 0, \"MALIGNANT\": 1}, inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFJR7_Myv3xD"
      },
      "source": [
        "#df.loc[df['column_name'] == some_value]\n",
        "#SampleSpace = ImagePaths.loc[ImagePaths['Pathology'] == 1]\n",
        "#Sampled_Data = SampleSpace.sample(n=650, replace = False)\n",
        "#NewTrainImagePaths= ImagePaths.append(Sampled_Data, ignore_index=True)   #df1.append(df2, ignore_index=True)\n",
        "#NewTrainImagePaths = NewTrainImagePaths.sample(frac = 1)\n",
        "#X = NewTrainImagePaths.ImagePath.values\n",
        "#y = NewTrainImagePaths.Pathology.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykXHwlktwFp_"
      },
      "source": [
        "#X = ImagePaths.ImagePath.values\n",
        "#y = ImagePaths.Pathology.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OIERZUxuMMj"
      },
      "source": [
        "#(xtrain, xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnVKRylUxL41"
      },
      "source": [
        "#TrainSetDataFrame = pd.DataFrame({'ImagePath': xtrain, 'Pathology': ytrain})\n",
        "#TestSetDataFrame = pd.DataFrame({'ImagePath': xtest, 'Pathology': ytest})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciya_Ou9xL8S"
      },
      "source": [
        "#TrainSetDataFrame.to_csv('/content/drive/MyDrive/TrainSetNew.csv')\n",
        "#TestSetDataFrame.to_csv('/content/drive/MyDrive/TestSetNew.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EPr-nC8bWgC",
        "outputId": "39baafb1-202f-45df-c769-f5205000eb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wQF6S0GyBsu"
      },
      "source": [
        "TrainDataFrame = pd.read_csv('/content/drive/MyDrive/TrainSetNew.csv')\n",
        "TestDataFrame = pd.read_csv('/content/drive/MyDrive/TestSetNew.csv')\n",
        "\n",
        "#TrainSet DataFrame\n",
        "xtrain = TrainDataFrame.ImagePath.values\n",
        "ytrain = TrainDataFrame.Pathology.values\n",
        "#TestSet DataFrame\n",
        "xtest = TestDataFrame.ImagePath.values\n",
        "ytest = TestDataFrame.Pathology.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBnuJXPoK1Fa",
        "outputId": "87ad7163-2f58-47c7-c943-a49bcabade53"
      },
      "source": [
        "#frequency of class in TrainSet\n",
        "(unique, counts) = np.unique(ytrain, return_counts=True)\n",
        "Trainfrequencies = np.asarray((unique, counts)).T\n",
        "Trainfrequencies"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0, 2576],\n",
              "       [   1, 2375]])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJMQumHe_bDm",
        "outputId": "d65f1088-9dc2-4964-c6d5-211be0d0fb76"
      },
      "source": [
        "#frequency of class in TestSet\n",
        "(unique, counts) = np.unique(ytest, return_counts=True)\n",
        "Testfrequencies = np.asarray((unique, counts)).T\n",
        "Testfrequencies"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0, 1080],\n",
              "       [   1, 1043]])"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwtf0-1EyQt"
      },
      "source": [
        "y = np.concatenate((ytrain, ytest))\n",
        "#print(ytrain.size, ytest.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQunL1g6U0Yq",
        "outputId": "f75bb0d9-52c2-4cc9-b994-d871d3b84172"
      },
      "source": [
        "labels = y\n",
        "# one hot encode\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "#print(labels)\n",
        "# pickle the label binarizer\n",
        "joblib.dump(lb, 'lb.pkl')\n",
        "print('Save the one-hot encoded binarized labels as a pickled file.')\n",
        "#print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save the one-hot encoded binarized labels as a pickled file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmlrAIGHWoUr",
        "outputId": "4e9d8a2a-cbc2-4c8c-ae98-ffe810b350f7"
      },
      "source": [
        "''' SEED Everything '''\n",
        "def seed_everything(SEED=42):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "SEED=42\n",
        "seed_everything(SEED=SEED)\n",
        "''' SEED Everything '''\n",
        "# set computation device\n",
        "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computation device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrxC0ynjV-uL"
      },
      "source": [
        "import sys\n",
        "sys.argv=['']\n",
        "del sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdAXqfz4V-3R"
      },
      "source": [
        "# image dataset module\n",
        "class DDSMimageDataset(Dataset):\n",
        "    def __init__(self, path, labels, tfms=None):\n",
        "        self.X = path\n",
        "        self.y = labels\n",
        "        # apply augmentations\n",
        "        if tfms == 0:  # if validating\n",
        "          self.aug = albumentations.Compose([\n",
        "                albumentations.Resize(224, 224, always_apply=True),    #224, 224,\n",
        "                albumentations.Normalize(mean=[0.485], std=[0.229], always_apply=True)\n",
        "            ])  #0.485, 0.456, 0.406;  0.229, 0.224, 0.225\n",
        "        else: # if training\n",
        "          self.aug = albumentations.Compose([\n",
        "                albumentations.Resize(224, 224, always_apply=True),\n",
        "                albumentations.HorizontalFlip(p=1.0),\n",
        "                albumentations.ShiftScaleRotate(shift_limit=0.3,scale_limit=0.3,rotate_limit=30,p=1.0),\n",
        "                albumentations.Normalize(mean=[0.485], std=[0.229], always_apply=True)\n",
        "            ])  #0.485, 0.456, 0.406;  0.229, 0.224, 0.225\n",
        "    def __len__(self):\n",
        "        return (len(self.X))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      image = Image.open(self.X[i])\n",
        "      #image = image.convert('RGB')\n",
        "      #image = self.aug(image=np.array(image))['image']\n",
        "      #image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "      #label = self.y[i]\n",
        "\n",
        "      image = self.aug(image=np.array(image))['image']\n",
        "      #image = torch.from_numpy(image) #.unsqueeze(0)\n",
        "      #image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "      label = self.y[i]\n",
        "      return torch.tensor(image[None, ...], dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
        "      #return torch.tensor(image, dtype=torch.float), torch.tensor(label, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXXvDnTXVjPL"
      },
      "source": [
        "BatchSize = 64 #32 #64 128;\n",
        "#train_data = DDSMimageDataset(xtrain, ytrain, tfms=1)\n",
        "test_data = DDSMimageDataset(xtest, ytest, tfms=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRFbicwFRCOZ"
      },
      "source": [
        "# dataloaders\n",
        "#trainloader = DataLoader(train_data, batch_size=BatchSize, shuffle=True)\n",
        "testloader = DataLoader(test_data, batch_size=BatchSize, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43sJL1TE22pA"
      },
      "source": [
        "def NewTrainLoader(BatchSize):\n",
        "  train_data = DDSMimageDataset(xtrain, ytrain, tfms=1)\n",
        "  trainloader = DataLoader(train_data, batch_size=BatchSize, shuffle=True)\n",
        "  return train_data, trainloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7ZuzUAK4Bq5"
      },
      "source": [
        "train_data, trainloader = NewTrainLoader(BatchSize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCc2l_usXCBq",
        "outputId": "c6f48665-e79d-46bd-f2e3-7a876995919f"
      },
      "source": [
        "# load the binarized labels\n",
        "print('Loading label binarizer...')\n",
        "lb = joblib.load('lb.pkl')\n",
        "print(lb.classes_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading label binarizer...\n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9R9mXOsknxH"
      },
      "source": [
        "class BuildingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, first_superblock, stride=1, internal_block=False):\n",
        "      super(BuildingBlock, self).__init__()\n",
        "      if first_superblock == True:\n",
        "        if internal_block == False:\n",
        "          self.layer_a = nn.Conv2d(in_channels, out_channels=in_channels, kernel_size=1, bias=False)\n",
        "          self.residual_downsample = nn.Sequential(nn.Conv2d(in_channels, in_channels*4, kernel_size=1, bias=False),\n",
        "                                                     nn.BatchNorm2d(in_channels*4))\n",
        "        else:\n",
        "          self.layer_a = nn.Conv2d(in_channels*4, out_channels=in_channels, kernel_size=1, bias=False)\n",
        "          self.residual_downsample = nn.Identity()\n",
        "      else:\n",
        "        if internal_block == False:\n",
        "          self.layer_a = nn.Conv2d(in_channels*2, out_channels=in_channels, kernel_size=1, stride=stride, bias=False)\n",
        "          self.residual_downsample= nn.Sequential(nn.Conv2d(in_channels*2, out_channels=in_channels*4, kernel_size=1, stride=stride, bias=False),\n",
        "                                                        nn.BatchNorm2d(in_channels*4))\n",
        "        else:\n",
        "            self.layer_a = nn.Conv2d(in_channels*4, out_channels=in_channels, stride=1, kernel_size=1, bias=False)\n",
        "            self.residual_downsample = nn.Identity()\n",
        "\n",
        "      self.bn_a = nn.BatchNorm2d(in_channels)\n",
        "      self.layer_b = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
        "      self.bn_b = nn.BatchNorm2d(in_channels)\n",
        "      self.layer_c = nn.Conv2d(in_channels, in_channels*4, kernel_size=1, bias=False)\n",
        "      self.bn_c = nn.BatchNorm2d(in_channels * 4)\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      #self.relu = nn.Tanh(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      output = self.layer_a(x)\n",
        "      output = self.bn_a(output)\n",
        "      output = self.relu(output)\n",
        "      #output = self.Tanh(output)\n",
        "\n",
        "      output = self.layer_b(output)\n",
        "      output = self.bn_b(output)\n",
        "      output = self.relu(output)\n",
        "      #output = self.Tanh(output)\n",
        "\n",
        "      output = self.layer_c(output)\n",
        "      output = self.bn_c(output)\n",
        "\n",
        "      residual = x\n",
        "      residual = self.residual_downsample(residual)\n",
        "      output += residual\n",
        "\n",
        "      output = self.relu(output)\n",
        "      #output = self.Tanh(output)\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5e3UaJ2CMkC"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, building_block, num_layers, num_classes=2):\n",
        "    super(ResNet, self).__init__()\n",
        "    BatchSizeNet = BatchSize #64 #32 #64 128\n",
        "    LinearOneOut = 2*2048 #64 #32;\n",
        "    NumLayers = len(num_layers)-1\n",
        "    self.internal_channels = BatchSizeNet\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.internal_channels, kernel_size=7,\n",
        "                           stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(self.internal_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    #self.superblock2 = self.make_superblock(building_block, 64, num_layers[0], first_superblock=True)\n",
        "    #self.superblock3 = self.make_superblock(building_block, 128, num_layers[1], stride=2)\n",
        "    #self.superblock4 = self.make_superblock(building_block, 256, num_layers[2], stride=2)\n",
        "    #self.superblock5 = self.make_superblock(building_block, 512, num_layers[3], stride=2)\n",
        "\n",
        "    self.superblock2 = self.make_superblock(building_block, BatchSizeNet, num_layers[0], first_superblock=True)\n",
        "    self.superblock3 = self.make_superblock(building_block, 2*BatchSizeNet, num_layers[1], stride=2)\n",
        "    self.superblock4 = self.make_superblock(building_block, 4*BatchSizeNet, num_layers[2], stride=2)\n",
        "    self.superblock5 = self.make_superblock(building_block, 8*BatchSizeNet, num_layers[3], stride=2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    #self.avgpool = nn.MaxPool2d(2,2) #nn.AdaptiveAvgPool2d((1, 1))\n",
        "    #self.fc1 = nn.Linear(512*4, 64)\n",
        "    #self.fc = nn.Linear((4*(2**NumLayers)*BatchSizeNet), 2)\n",
        "    self.fc1 = nn.Sequential(nn.Linear((4*(2**NumLayers)*BatchSizeNet), LinearOneOut), nn.BatchNorm1d(LinearOneOut), nn.ReLU(inplace = True))\n",
        "    #self.fc2 = nn.Linear(256, 2)\n",
        "    self.fc2 =  nn.Sequential(nn.Linear(LinearOneOut, 2)) #, nn.LogSoftmax(dim=1) )\n",
        "    self.init_weights()\n",
        "\n",
        "  def make_superblock(self, building_block, in_channels, num_blocks, stride=1, first_superblock=False,\n",
        "                        internal_block=True):\n",
        "    if first_superblock == True:\n",
        "      layers = [building_block(in_channels, first_superblock)]\n",
        "      for i in range(num_blocks-1):\n",
        "        layers.append(building_block(in_channels, first_superblock, internal_block=True))\n",
        "    else:\n",
        "      layers = [building_block(in_channels, stride=stride, first_superblock=False, internal_block=False)]\n",
        "      for i in range(num_blocks-1):\n",
        "        layers.append(building_block(in_channels, first_superblock=False, internal_block=True))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    #x = self.Tanh(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.superblock2(x)\n",
        "    x = self.superblock3(x)\n",
        "    x = self.superblock4(x)\n",
        "    x = self.superblock5(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    #print(x.shape)\n",
        "    #x = self.fc(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return F.softmax(x, dim=1)\n",
        "    #return x\n",
        "    #return F.sigmoid(x)\n",
        "\n",
        "  def init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0YxlRqxGtdY"
      },
      "source": [
        " def build_resnet(backbone):\n",
        "    if backbone == 'ResNet50':\n",
        "      num_layers = [3, 4, 6, 3]\n",
        "    elif backbone == 'ResNet101':\n",
        "      num_layers = [3, 4, 23, 3]\n",
        "    elif backbone == 'ResNet152':\n",
        "      num_layers = [3, 8, 36, 3]\n",
        "    elif backbone == 'ResNet18':\n",
        "      num_layers = [2, 2, 2, 2]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "    model = ResNet(BuildingBlock, num_layers)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_weCHLuan_t"
      },
      "source": [
        "def model(pretrained, requires_grad):\n",
        "    model = models.resnet18(progress=True, pretrained=pretrained)\n",
        "    #input layer of ResNet.....\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "    # freeze hidden layers\n",
        "    if requires_grad == False:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    # train the hidden layers\n",
        "    elif requires_grad == True:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "    # make the classification layer learnable\n",
        "    numFcInputs = model.fc.in_features\n",
        "    print(numFcInputs)\n",
        "    model.fc = nn.Sequential(nn.Linear(numFcInputs, len(lb.classes_)), nn.Softmax(dim=1))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mxs1q7ICm8p",
        "outputId": "9a01ca04-0ba6-440f-cc10-34d1fbccb97b"
      },
      "source": [
        "#model = model(pretrained= True, requires_grad = False).to(device) #ResNet18 -- DeepModel\n",
        "model = build_resnet(backbone='ResNet50').to(device)\n",
        "#print(model)\n",
        "print(summary(model, (1, 1024, 1024))) #, device = 'cpu'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 512, 512]           3,136\n",
            "       BatchNorm2d-2         [-1, 64, 512, 512]             128\n",
            "              ReLU-3         [-1, 64, 512, 512]               0\n",
            "         MaxPool2d-4         [-1, 64, 256, 256]               0\n",
            "            Conv2d-5         [-1, 64, 256, 256]           4,096\n",
            "       BatchNorm2d-6         [-1, 64, 256, 256]             128\n",
            "              ReLU-7         [-1, 64, 256, 256]               0\n",
            "            Conv2d-8         [-1, 64, 256, 256]          36,864\n",
            "       BatchNorm2d-9         [-1, 64, 256, 256]             128\n",
            "             ReLU-10         [-1, 64, 256, 256]               0\n",
            "           Conv2d-11        [-1, 256, 256, 256]          16,384\n",
            "      BatchNorm2d-12        [-1, 256, 256, 256]             512\n",
            "           Conv2d-13        [-1, 256, 256, 256]          16,384\n",
            "      BatchNorm2d-14        [-1, 256, 256, 256]             512\n",
            "             ReLU-15        [-1, 256, 256, 256]               0\n",
            "    BuildingBlock-16        [-1, 256, 256, 256]               0\n",
            "           Conv2d-17         [-1, 64, 256, 256]          16,384\n",
            "      BatchNorm2d-18         [-1, 64, 256, 256]             128\n",
            "             ReLU-19         [-1, 64, 256, 256]               0\n",
            "           Conv2d-20         [-1, 64, 256, 256]          36,864\n",
            "      BatchNorm2d-21         [-1, 64, 256, 256]             128\n",
            "             ReLU-22         [-1, 64, 256, 256]               0\n",
            "           Conv2d-23        [-1, 256, 256, 256]          16,384\n",
            "      BatchNorm2d-24        [-1, 256, 256, 256]             512\n",
            "         Identity-25        [-1, 256, 256, 256]               0\n",
            "             ReLU-26        [-1, 256, 256, 256]               0\n",
            "    BuildingBlock-27        [-1, 256, 256, 256]               0\n",
            "           Conv2d-28         [-1, 64, 256, 256]          16,384\n",
            "      BatchNorm2d-29         [-1, 64, 256, 256]             128\n",
            "             ReLU-30         [-1, 64, 256, 256]               0\n",
            "           Conv2d-31         [-1, 64, 256, 256]          36,864\n",
            "      BatchNorm2d-32         [-1, 64, 256, 256]             128\n",
            "             ReLU-33         [-1, 64, 256, 256]               0\n",
            "           Conv2d-34        [-1, 256, 256, 256]          16,384\n",
            "      BatchNorm2d-35        [-1, 256, 256, 256]             512\n",
            "         Identity-36        [-1, 256, 256, 256]               0\n",
            "             ReLU-37        [-1, 256, 256, 256]               0\n",
            "    BuildingBlock-38        [-1, 256, 256, 256]               0\n",
            "           Conv2d-39        [-1, 128, 128, 128]          32,768\n",
            "      BatchNorm2d-40        [-1, 128, 128, 128]             256\n",
            "             ReLU-41        [-1, 128, 128, 128]               0\n",
            "           Conv2d-42        [-1, 128, 128, 128]         147,456\n",
            "      BatchNorm2d-43        [-1, 128, 128, 128]             256\n",
            "             ReLU-44        [-1, 128, 128, 128]               0\n",
            "           Conv2d-45        [-1, 512, 128, 128]          65,536\n",
            "      BatchNorm2d-46        [-1, 512, 128, 128]           1,024\n",
            "           Conv2d-47        [-1, 512, 128, 128]         131,072\n",
            "      BatchNorm2d-48        [-1, 512, 128, 128]           1,024\n",
            "             ReLU-49        [-1, 512, 128, 128]               0\n",
            "    BuildingBlock-50        [-1, 512, 128, 128]               0\n",
            "           Conv2d-51        [-1, 128, 128, 128]          65,536\n",
            "      BatchNorm2d-52        [-1, 128, 128, 128]             256\n",
            "             ReLU-53        [-1, 128, 128, 128]               0\n",
            "           Conv2d-54        [-1, 128, 128, 128]         147,456\n",
            "      BatchNorm2d-55        [-1, 128, 128, 128]             256\n",
            "             ReLU-56        [-1, 128, 128, 128]               0\n",
            "           Conv2d-57        [-1, 512, 128, 128]          65,536\n",
            "      BatchNorm2d-58        [-1, 512, 128, 128]           1,024\n",
            "         Identity-59        [-1, 512, 128, 128]               0\n",
            "             ReLU-60        [-1, 512, 128, 128]               0\n",
            "    BuildingBlock-61        [-1, 512, 128, 128]               0\n",
            "           Conv2d-62        [-1, 128, 128, 128]          65,536\n",
            "      BatchNorm2d-63        [-1, 128, 128, 128]             256\n",
            "             ReLU-64        [-1, 128, 128, 128]               0\n",
            "           Conv2d-65        [-1, 128, 128, 128]         147,456\n",
            "      BatchNorm2d-66        [-1, 128, 128, 128]             256\n",
            "             ReLU-67        [-1, 128, 128, 128]               0\n",
            "           Conv2d-68        [-1, 512, 128, 128]          65,536\n",
            "      BatchNorm2d-69        [-1, 512, 128, 128]           1,024\n",
            "         Identity-70        [-1, 512, 128, 128]               0\n",
            "             ReLU-71        [-1, 512, 128, 128]               0\n",
            "    BuildingBlock-72        [-1, 512, 128, 128]               0\n",
            "           Conv2d-73        [-1, 128, 128, 128]          65,536\n",
            "      BatchNorm2d-74        [-1, 128, 128, 128]             256\n",
            "             ReLU-75        [-1, 128, 128, 128]               0\n",
            "           Conv2d-76        [-1, 128, 128, 128]         147,456\n",
            "      BatchNorm2d-77        [-1, 128, 128, 128]             256\n",
            "             ReLU-78        [-1, 128, 128, 128]               0\n",
            "           Conv2d-79        [-1, 512, 128, 128]          65,536\n",
            "      BatchNorm2d-80        [-1, 512, 128, 128]           1,024\n",
            "         Identity-81        [-1, 512, 128, 128]               0\n",
            "             ReLU-82        [-1, 512, 128, 128]               0\n",
            "    BuildingBlock-83        [-1, 512, 128, 128]               0\n",
            "           Conv2d-84          [-1, 256, 64, 64]         131,072\n",
            "      BatchNorm2d-85          [-1, 256, 64, 64]             512\n",
            "             ReLU-86          [-1, 256, 64, 64]               0\n",
            "           Conv2d-87          [-1, 256, 64, 64]         589,824\n",
            "      BatchNorm2d-88          [-1, 256, 64, 64]             512\n",
            "             ReLU-89          [-1, 256, 64, 64]               0\n",
            "           Conv2d-90         [-1, 1024, 64, 64]         262,144\n",
            "      BatchNorm2d-91         [-1, 1024, 64, 64]           2,048\n",
            "           Conv2d-92         [-1, 1024, 64, 64]         524,288\n",
            "      BatchNorm2d-93         [-1, 1024, 64, 64]           2,048\n",
            "             ReLU-94         [-1, 1024, 64, 64]               0\n",
            "    BuildingBlock-95         [-1, 1024, 64, 64]               0\n",
            "           Conv2d-96          [-1, 256, 64, 64]         262,144\n",
            "      BatchNorm2d-97          [-1, 256, 64, 64]             512\n",
            "             ReLU-98          [-1, 256, 64, 64]               0\n",
            "           Conv2d-99          [-1, 256, 64, 64]         589,824\n",
            "     BatchNorm2d-100          [-1, 256, 64, 64]             512\n",
            "            ReLU-101          [-1, 256, 64, 64]               0\n",
            "          Conv2d-102         [-1, 1024, 64, 64]         262,144\n",
            "     BatchNorm2d-103         [-1, 1024, 64, 64]           2,048\n",
            "        Identity-104         [-1, 1024, 64, 64]               0\n",
            "            ReLU-105         [-1, 1024, 64, 64]               0\n",
            "   BuildingBlock-106         [-1, 1024, 64, 64]               0\n",
            "          Conv2d-107          [-1, 256, 64, 64]         262,144\n",
            "     BatchNorm2d-108          [-1, 256, 64, 64]             512\n",
            "            ReLU-109          [-1, 256, 64, 64]               0\n",
            "          Conv2d-110          [-1, 256, 64, 64]         589,824\n",
            "     BatchNorm2d-111          [-1, 256, 64, 64]             512\n",
            "            ReLU-112          [-1, 256, 64, 64]               0\n",
            "          Conv2d-113         [-1, 1024, 64, 64]         262,144\n",
            "     BatchNorm2d-114         [-1, 1024, 64, 64]           2,048\n",
            "        Identity-115         [-1, 1024, 64, 64]               0\n",
            "            ReLU-116         [-1, 1024, 64, 64]               0\n",
            "   BuildingBlock-117         [-1, 1024, 64, 64]               0\n",
            "          Conv2d-118          [-1, 256, 64, 64]         262,144\n",
            "     BatchNorm2d-119          [-1, 256, 64, 64]             512\n",
            "            ReLU-120          [-1, 256, 64, 64]               0\n",
            "          Conv2d-121          [-1, 256, 64, 64]         589,824\n",
            "     BatchNorm2d-122          [-1, 256, 64, 64]             512\n",
            "            ReLU-123          [-1, 256, 64, 64]               0\n",
            "          Conv2d-124         [-1, 1024, 64, 64]         262,144\n",
            "     BatchNorm2d-125         [-1, 1024, 64, 64]           2,048\n",
            "        Identity-126         [-1, 1024, 64, 64]               0\n",
            "            ReLU-127         [-1, 1024, 64, 64]               0\n",
            "   BuildingBlock-128         [-1, 1024, 64, 64]               0\n",
            "          Conv2d-129          [-1, 256, 64, 64]         262,144\n",
            "     BatchNorm2d-130          [-1, 256, 64, 64]             512\n",
            "            ReLU-131          [-1, 256, 64, 64]               0\n",
            "          Conv2d-132          [-1, 256, 64, 64]         589,824\n",
            "     BatchNorm2d-133          [-1, 256, 64, 64]             512\n",
            "            ReLU-134          [-1, 256, 64, 64]               0\n",
            "          Conv2d-135         [-1, 1024, 64, 64]         262,144\n",
            "     BatchNorm2d-136         [-1, 1024, 64, 64]           2,048\n",
            "        Identity-137         [-1, 1024, 64, 64]               0\n",
            "            ReLU-138         [-1, 1024, 64, 64]               0\n",
            "   BuildingBlock-139         [-1, 1024, 64, 64]               0\n",
            "          Conv2d-140          [-1, 256, 64, 64]         262,144\n",
            "     BatchNorm2d-141          [-1, 256, 64, 64]             512\n",
            "            ReLU-142          [-1, 256, 64, 64]               0\n",
            "          Conv2d-143          [-1, 256, 64, 64]         589,824\n",
            "     BatchNorm2d-144          [-1, 256, 64, 64]             512\n",
            "            ReLU-145          [-1, 256, 64, 64]               0\n",
            "          Conv2d-146         [-1, 1024, 64, 64]         262,144\n",
            "     BatchNorm2d-147         [-1, 1024, 64, 64]           2,048\n",
            "        Identity-148         [-1, 1024, 64, 64]               0\n",
            "            ReLU-149         [-1, 1024, 64, 64]               0\n",
            "   BuildingBlock-150         [-1, 1024, 64, 64]               0\n",
            "          Conv2d-151          [-1, 512, 32, 32]         524,288\n",
            "     BatchNorm2d-152          [-1, 512, 32, 32]           1,024\n",
            "            ReLU-153          [-1, 512, 32, 32]               0\n",
            "          Conv2d-154          [-1, 512, 32, 32]       2,359,296\n",
            "     BatchNorm2d-155          [-1, 512, 32, 32]           1,024\n",
            "            ReLU-156          [-1, 512, 32, 32]               0\n",
            "          Conv2d-157         [-1, 2048, 32, 32]       1,048,576\n",
            "     BatchNorm2d-158         [-1, 2048, 32, 32]           4,096\n",
            "          Conv2d-159         [-1, 2048, 32, 32]       2,097,152\n",
            "     BatchNorm2d-160         [-1, 2048, 32, 32]           4,096\n",
            "            ReLU-161         [-1, 2048, 32, 32]               0\n",
            "   BuildingBlock-162         [-1, 2048, 32, 32]               0\n",
            "          Conv2d-163          [-1, 512, 32, 32]       1,048,576\n",
            "     BatchNorm2d-164          [-1, 512, 32, 32]           1,024\n",
            "            ReLU-165          [-1, 512, 32, 32]               0\n",
            "          Conv2d-166          [-1, 512, 32, 32]       2,359,296\n",
            "     BatchNorm2d-167          [-1, 512, 32, 32]           1,024\n",
            "            ReLU-168          [-1, 512, 32, 32]               0\n",
            "          Conv2d-169         [-1, 2048, 32, 32]       1,048,576\n",
            "     BatchNorm2d-170         [-1, 2048, 32, 32]           4,096\n",
            "        Identity-171         [-1, 2048, 32, 32]               0\n",
            "            ReLU-172         [-1, 2048, 32, 32]               0\n",
            "   BuildingBlock-173         [-1, 2048, 32, 32]               0\n",
            "          Conv2d-174          [-1, 512, 32, 32]       1,048,576\n",
            "     BatchNorm2d-175          [-1, 512, 32, 32]           1,024\n",
            "            ReLU-176          [-1, 512, 32, 32]               0\n",
            "          Conv2d-177          [-1, 512, 32, 32]       2,359,296\n",
            "     BatchNorm2d-178          [-1, 512, 32, 32]           1,024\n",
            "            ReLU-179          [-1, 512, 32, 32]               0\n",
            "          Conv2d-180         [-1, 2048, 32, 32]       1,048,576\n",
            "     BatchNorm2d-181         [-1, 2048, 32, 32]           4,096\n",
            "        Identity-182         [-1, 2048, 32, 32]               0\n",
            "            ReLU-183         [-1, 2048, 32, 32]               0\n",
            "   BuildingBlock-184         [-1, 2048, 32, 32]               0\n",
            "AdaptiveAvgPool2d-185           [-1, 2048, 1, 1]               0\n",
            "          Linear-186                 [-1, 4096]       8,392,704\n",
            "     BatchNorm1d-187                 [-1, 4096]           8,192\n",
            "            ReLU-188                 [-1, 4096]               0\n",
            "          Linear-189                    [-1, 2]           8,194\n",
            "================================================================\n",
            "Total params: 31,910,850\n",
            "Trainable params: 31,910,850\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 4.00\n",
            "Forward/backward pass size (MB): 6376.11\n",
            "Params size (MB): 121.73\n",
            "Estimated Total Size (MB): 6501.84\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO4uy_jI3rIg"
      },
      "source": [
        "#images, labels = next(iter(testloader))\n",
        "#grid = torchvision.utils.make_grid(images*0.229+0.485 , nrow=4)   #0.485;   0.229,\n",
        "#writer.add_image('image', grid)\n",
        "#writer.add_graph(model, images.to(device).clone().detach())\n",
        "#writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuY1I3-OjmTh"
      },
      "source": [
        "#%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2FBzYeIjp70"
      },
      "source": [
        "#tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSZ3tBs6KKEl"
      },
      "source": [
        "#print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "971nAFBRIKOs"
      },
      "source": [
        "#for name, param in model.named_parameters():\n",
        "#  print(name, param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUCP771qjrTy"
      },
      "source": [
        "def conMatrix(target, pred):\n",
        "  nb_classes = 2   #confusion_matrix\n",
        "  conf_Vector = []  #pred.view(-1), target.view(-1)\n",
        "  conf_Vector = torch.tensor(conf_Vector, dtype=torch.long).to(device)\n",
        "  conf_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "  for t, p in zip(target, pred):\n",
        "    conf_matrix[t, p] += 1\n",
        "  #conf_matrix =confusion_matrix(pred.view(-1), target.view(-1))\n",
        "  for i in range(0, (2)):\n",
        "    if i == 0:\n",
        "      conf_Vector = torch.cat((conf_Vector, torch.diag(conf_matrix,i).to(device)), 0)\n",
        "    else:\n",
        "      conf_Vector = torch.cat((conf_Vector, torch.diag(conf_matrix,-i).to(device)), 0)\n",
        "      conf_Vector = torch.cat((conf_Vector, torch.diag(conf_matrix,+i).to(device)), 0)\n",
        "  confVector = (conf_Vector.data).cpu().numpy()\n",
        "  print('    Evaluation Matrix')\n",
        "  return confVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBnJaW8N6uMY"
      },
      "source": [
        "def L1_L2Reg(model):\n",
        "  RegLoss = 0\n",
        "  LamdaValue = 0.0 #1e-6\n",
        "  for param in model.parameters():\n",
        "    RegLoss += torch.sum(abs(param))\n",
        "  return RegLoss*LamdaValue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB_DvuISd7j_"
      },
      "source": [
        "learnRate = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()  # loss function\n",
        "WeightDecay = 0.000001 #0.001  #0.00005\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learnRate, betas=(0.9, 0.999),\n",
        "                       eps=1e-8,weight_decay=WeightDecay, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNCHcs9WXH6_"
      },
      "source": [
        "# training function\n",
        "def fit(model, train_dataloader, epoch):\n",
        "    print('    Training.....', 'Epoch #', epoch)\n",
        "    model.train()\n",
        "    DataSize = 0\n",
        "    totalItr=int(len(train_data)/train_dataloader.batch_size)\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    for i, data in tqdm(enumerate(train_dataloader), total=int(len(train_data)/train_dataloader.batch_size)):\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        DataSize+=target.size(0)  #print(DataSize)\n",
        "        outputs = model.forward(data)\n",
        "        error = criterion(outputs, target) #+ L1_L2Reg(model) #.unsqueeze(1))\n",
        "        train_running_loss += (error.item()*target.size(0))\n",
        "        #_, preds = torch.max(outputs.data, 1)  #torch.argmax(a, dim=1)\n",
        "        #torch.argmax(a, dim=1)\n",
        "        #preds = torch.argmax(F.softmax(outputs.data, dim=1), dim=1) #outputs.argmax(dim=1, keepdim=True)\n",
        "        preds = torch.argmax(outputs.data, 1)\n",
        "        #print(preds.dtype, target.dtype)\n",
        "        train_running_correct += (preds == target).sum().item()\n",
        "        #optimizer.zero_grad()\n",
        "        loss = criterion(outputs, target) + L1_L2Reg(model)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        del data, target, outputs, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    train_loss = train_running_loss/DataSize #len(train_dataloader.dataset)\n",
        "    train_accuracy = 100. * train_running_correct/DataSize #len(train_dataloader.dataset)\n",
        "    #print(DataSize, totalItr*64, 100.*train_running_correct/DataSize, 100.*train_running_correct/(totalItr*64))\n",
        "    #print(DataSize)\n",
        "    #print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}\")\n",
        "    return train_loss, train_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP31DXXrXH-h"
      },
      "source": [
        "#validation function\n",
        "def validate(model, test_dataloader):\n",
        "    print('    Validating')\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0   #i = torch.tensor([i]).float(), a = torch.cat((a, i), 0)\n",
        "    val_running_correct = 0   #torch.tensor(label, dtype=torch.long)\n",
        "    CofTarget, ConfPreds, ProbScores = [], [], []\n",
        "    CofTarget = torch.tensor(CofTarget, dtype=torch.long).to(device)\n",
        "    ConfPreds = torch.tensor(ConfPreds, dtype=torch.long).to(device)\n",
        "    ProbScores = torch.tensor(ProbScores, dtype=torch.long).to(device)\n",
        "    DataSize = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(test_dataloader), total=int(len(test_data)/test_dataloader.batch_size)):\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "            outputs = model.forward(data)\n",
        "            loss = criterion(outputs, target) #.unsqueeze(1))\n",
        "            DataSize += target.size(0)  # print(DataSize)\n",
        "            val_running_loss += (loss.item()*target.size(0))\n",
        "            #_, preds = torch.max(outputs.data, 1)\n",
        "            #preds = outputs.argmax(dim=1, keepdim=True)  #tensor_array.cpu().detach().numpy()\n",
        "            #preds = torch.argmax(F.softmax(outputs.data, dim=1), dim=1)\n",
        "            preds = torch.argmax(outputs.data, 1)\n",
        "            ProbScore = outputs.data[:,1]\n",
        "            val_running_correct += (preds == target).sum().item()\n",
        "\n",
        "            CofTarget = torch.cat((CofTarget, target), 0) #.to('cpu').numpy()]  #a = torch.cat((a, i), 0)\n",
        "            ConfPreds = torch.cat((ConfPreds, preds), 0)  #[ConfPreds, preds.to('cpu').numpy()]\n",
        "            ProbScores = torch.cat((ProbScores, ProbScore), 0)\n",
        "            del data, target, outputs, preds, ProbScore\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        val_loss = val_running_loss/DataSize #len(test_dataloader.dataset)\n",
        "        val_accuracy = 100. * val_running_correct/DataSize #len(test_dataloader.dataset)\n",
        "        #print(CofTarget.size, ConfPreds.size)\n",
        "        #return roc_auc_score(y_true, y_pred)\n",
        "        #print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n",
        "        return val_loss, val_accuracy, CofTarget, ConfPreds, ProbScores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-O1xFmL2vqp"
      },
      "source": [
        "#AUC-ROC evaluation....L1_L2Reg(model):\n",
        "def ROC_AUC_Evaluation(CofTarget, ConfPreds, ProbScores):\n",
        "  ConfVector = conMatrix(CofTarget, ConfPreds)\n",
        "  Ytarget = CofTarget.cpu().numpy()\n",
        "  Ypreds = ConfPreds.cpu().numpy()\n",
        "  Yscore = ProbScores.cpu().numpy()\n",
        "  RoC_Value = roc_auc_score(Ytarget,Yscore)\n",
        "  FPRValue, TPRVlaue, _ = roc_curve(Ytarget, Yscore)\n",
        "  return ConfVector, RoC_Value, FPRValue, TPRVlaue\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T9iOBXWXICE"
      },
      "source": [
        "# lists to store per-epoch loss and accuracy values\n",
        "train_loss, train_accuracy = [], []\n",
        "val_loss, val_accuracy = [], []\n",
        "ConfVector, RocAuC = [], []\n",
        "fprValue, tprValue = [], []\n",
        "start = time.time()\n",
        "for epoch in range(1,11): #epochs):\n",
        "    #print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "\n",
        "    #if np.remainder(epoch, 5) == 0:\n",
        "      #train_data, trainloader = NewTrainLoader(BatchSize)\n",
        "      #learnRate = alphaZero/(1+DecayRate*epoch)\n",
        "      #optimizer = optim.Adam(model.parameters(), lr=learnRate, betas=(0.9, 0.999),\n",
        "      #                      eps=1e-8, weight_decay=0.0000) #, amsgrad=True)\n",
        "\n",
        "    train_epoch_loss, train_epoch_accuracy = fit(model, trainloader, epoch)\n",
        "    writer.add_scalar(\"Loss/training\", train_epoch_loss, epoch)\n",
        "\n",
        "    val_epoch_loss, val_epoch_accuracy, CofTarget, ConfPreds, ProbScores = validate(model, testloader)\n",
        "    writer.add_scalar(\"Loss/Validation\", val_epoch_loss, epoch)\n",
        "\n",
        "    #del train_data, trainloader\n",
        "    #train_data, trainloader = NewTrainLoader(BatchSize)\n",
        "    Conf_Vector, ROC_AUC, FPRValue, TPRValue = ROC_AUC_Evaluation(CofTarget, ConfPreds, ProbScores)\n",
        "    #print(TPRVlaue.shape)\n",
        "    RocAuC.append(ROC_AUC)\n",
        "    fprValue.append(FPRValue)\n",
        "    tprValue.append(TPRValue)\n",
        "    ConfVector.append(Conf_Vector)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracy.append(train_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f}\")\n",
        "    print(f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f}')\n",
        "    print(Conf_Vector)\n",
        "    print(ROC_AUC) #, TPRValue)\n",
        "\n",
        "writer.flush()\n",
        "end = time.time()\n",
        "print(f\"Training time: {(end-start)/60:.3f} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmXGmxvmXIEN"
      },
      "source": [
        "print('Saving loss and accuracy plots...')\n",
        "# accuracy plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_accuracy, color='green', label='train accuracy')\n",
        "plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "#plt.savefig(f\"../outputs/{acc_plot_name}.png\")\n",
        "plt.show()\n",
        "# loss plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color='orange', label='train loss')\n",
        "plt.plot(val_loss, color='red', label='validataion loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "#plt.savefig(f\"../outputs/{loss_plot_name}.png\")\n",
        "plt.show()\n",
        "\n",
        "# serialize the model to disk\n",
        "print('Saving model...')\n",
        "#torch.save(model.state_dict(), f\"../outputs/{model_name}.pth\")\n",
        "\n",
        "print('TRAINING COMPLETE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWC_QJB_1gkY"
      },
      "source": [
        "#print(RocAuC)\n",
        "IndexRocAUC = np.argmax(RocAuC)\n",
        "print(IndexRocAUC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TsdP7bL1jd-"
      },
      "source": [
        "#ROC-AUC plot\n",
        "torch.set_printoptions(precision=2)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(FPRValue, TPRValue, label=\"ROC Curve, area = \"+str('%.4f' % RocAuC[IndexRocAUC]))\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JiwNEShiyJo"
      },
      "source": [
        "DtFrm = pd.DataFrame({'tAcc': train_accuracy, 'tLoss': train_loss, 'vAcc': val_accuracy, 'vLoss': val_loss})\n",
        "DtFrm.to_csv('/content/drive/MyDrive/ResNet$18/ResNetSim#10.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZqYc_r8N4o2"
      },
      "source": [
        "DtFrmConf = pd.DataFrame (ConfVector,columns=['TP', 'TN', 'FP', 'FN'])\n",
        "DtFrmConf.to_csv('/content/drive/MyDrive/ResNet$18/Confusion#10.csv') #test[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNdpJKV58dXR"
      },
      "source": [
        "# saving RocAuC,fprValue,tprVlaue\n",
        "#DtFrmAuC = pd.DataFrame(RocAuC, tprValue, fprValue, columns=['RoCAuC','TPR', 'FPR'])\n",
        "#DtFrmAuC.to_csv('/content/drive/MyDrive/ResNetModel$6/RoCAuC#70.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVO5tJ_m3UAz"
      },
      "source": [
        "# save model checkpoint\n",
        "torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(), 'loss': criterion,\n",
        "            }, '/content/drive/MyDrive/ResNet$18/ResNet#10.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}